# -*- coding: utf-8 -*-
"""Lr_1_task_5.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LOOLCMzsyBMBZ3MwCUxVW8-MO9JRfXoU
"""

import pandas as pd
df = pd.read_csv('data_metrics.csv')
df.head()
thresh = 0.5
df['predicted_RF'] = (df.model_RF >= 0.5).astype('int')
df['predicted_LR'] = (df.model_LR >= 0.5).astype('int')
print(df.head())

from sklearn.metrics import confusion_matrix
rf_cm = confusion_matrix(df['actual_label'], df['predicted_RF'])
def perehvatova_confusion_matrix(y_true, y_pred):
   TP = sum((y_true.astype(int) == 1) & (y_pred.astype(int) == 1))
   FN = sum((y_true == 1) & (y_pred == 0))
   FP = sum((y_true == 0) & (y_pred == 1))
   TN = sum((y_true.astype(int) == 0) & (y_pred.astype(int) == 0))

   return np.array([[TP, FP], [FN, TN]], dtype=int)



print(rf_cm)
print(custom_cm)

from sklearn.metrics import accuracy_score

def find_conf_matrix_values(y_true, y_pred):
    TP, FN, FP, TN = 0, 0, 0, 0
    for i in range(len(y_true)):
        if y_true[i] == 1 and y_pred[i] == 1:
            TP += 1
        elif y_true[i] == 1 and y_pred[i] == 0:
            FN += 1
        elif y_true[i] == 0 and y_pred[i] == 1:
            FP += 1
        else:
            TN += 1
    return TP, FN, FP, TN

def perehv_accuracy_score(y_true, y_pred):
    TP, FN, FP, TN = find_conf_matrix_values(y_true, y_pred)
    accuracy = (TP + TN) / (TP + TN + FP + FN)
    return accuracy

accuracy_RF = accuracy_score(df.actual_label.values, df.predicted_RF.values)

assert perehv_accuracy_score(df.actual_label.values, df.predicted_RF.values) == accuracy_RF
assert perehv_accuracy_score(df.actual_label.values, df.predicted_LR.values) == accuracy_score(df.actual_label.values, df.predicted_LR.values)

print('Accuracy RF: %.3f' % (accuracy_RF))
print('Accuracy LR: %.3f' % (accuracy_score(df.actual_label.values, df.predicted_LR.values)))

from sklearn.metrics import recall_score

def find_conf_matrix_values(y_true, y_pred):
    TP, FN, FP, TN = 0, 0, 0, 0
    for i in range(len(y_true)):
        if y_true[i] == 1 and y_pred[i] == 1:
            TP += 1
        elif y_true[i] == 1 and y_pred[i] == 0:
            FN += 1
        elif y_true[i] == 0 and y_pred[i] == 1:
            FP += 1
        else:
            TN += 1
    return TP, FN, FP, TN

def perehv_recall_score(y_true, y_pred):
    TP, FN, FP, TN = find_conf_matrix_values(y_true, y_pred)
    recall = TP / (TP + FN)  # Recall calculation
    return recall

recall_RF = recall_score(df.actual_label.values, df.predicted_RF.values)

assert perehv_recall_score(df.actual_label.values, df.predicted_RF.values) == recall_RF, 'perehv_recall_score failed on RF'
assert perehv_recall_score(df.actual_label.values, df.predicted_LR.values) == recall_score(df.actual_label.values, df.predicted_LR.values), 'perehv_recall_score failed on LR'

print('Recall RF: %.3f' % recall_RF)
print('Recall LR: %.3f' % perehv_recall_score(df.actual_label.values, df.predicted_LR.values))

from sklearn.metrics import precision_score

def find_conf_matrix_values(y_true, y_pred):
    TP, FN, FP, TN = 0, 0, 0, 0
    for i in range(len(y_true)):
        if y_true[i] == 1 and y_pred[i] == 1:
            TP += 1
        elif y_true[i] == 1 and y_pred[i] == 0:
            FN += 1
        elif y_true[i] == 0 and y_pred[i] == 1:
            FP += 1
        else:
            TN += 1
    return TP, FN, FP, TN

def perehv_precision_score(y_true, y_pred):
    TP, FN, FP, TN = find_conf_matrix_values(y_true, y_pred)
    precision = TP / (TP + FP)  # Precision calculation
    return precision

precision_RF = precision_score(df.actual_label.values, df.predicted_RF.values)

assert perehv_precision_score(df.actual_label.values, df.predicted_RF.values) == precision_RF, 'perehv_precision_score failed on RF'
assert perehv_precision_score(df.actual_label.values, df.predicted_LR.values) == precision_score(df.actual_label.values, df.predicted_LR.values), 'perehv_precision_score failed on LR'

print('Precision RF: %.3f' % precision_RF)
print('Precision LR: %.3f' % perehv_precision_score(df.actual_label.values, df.predicted_LR.values))

from sklearn.metrics import f1_score

def perehv_f1_score(y_true, y_pred):
    recall = perehv_recall_score(y_true, y_pred)
    precision = perehv_precision_score(y_true, y_pred)
    f1 = 2 * (precision * recall) / (precision + recall)  # F1 score calculation
    return f1

f1_RF = f1_score(df.actual_label.values, df.predicted_RF.values)

assert perehv_f1_score(df.actual_label.values, df.predicted_RF.values) == f1_RF, 'perehv_f1_score failed on RF'
assert perehv_f1_score(df.actual_label.values, df.predicted_LR.values) == f1_score(df.actual_label.values, df.predicted_LR.values), 'perehv_f1_score failed on LR'

print('F1 RF: %.3f' % f1_RF)
print('F1 LR: %.3f' % perehv_f1_score(df.actual_label.values, df.predicted_LR.values))

# Scores with threshold = 0.5
print('Accuracy RF: %.3f' % perehv_accuracy_score(df.actual_label.values, df.predicted_RF.values))
print('Recall RF: %.3f' % perehv_recall_score(df.actual_label.values, df.predicted_RF.values))
print('Precision RF: %.3f' % perehv_precision_score(df.actual_label.values, df.predicted_RF.values))
print('F1 RF: %.3f' % perehv_f1_score(df.actual_label.values, df.predicted_RF.values))
print('')

# Scores with threshold = 0.25
print('Accuracy RF: %.3f' % perehv_accuracy_score(df.actual_label.values, (df.model_RF >= 0.25).astype('int').values))
print('Recall RF: %.3f' % perehv_recall_score(df.actual_label.values, (df.model_RF >= 0.25).astype('int').values))
print('Precision RF: %.3f' % perehv_precision_score(df.actual_label.values, (df.model_RF >= 0.25).astype('int').values))
print('F1 RF: %.3f' % perehv_f1_score(df.actual_label.values, (df.model_RF >= 0.25).astype('int').values))


from sklearn.metrics import roc_curve
import matplotlib.pyplot as plt

fpr_RF, tpr_RF, thresholds_RF = roc_curve(df.actual_label.values, df.model_RF.values)
fpr_LR, tpr_LR, thresholds_LR = roc_curve(df.actual_label.values, df.model_LR.values)

plt.plot(fpr_RF, tpr_RF, 'r-', label='RF')
plt.plot(fpr_LR, tpr_LR, 'b-', label='LR')
plt.plot([0, 1], [0, 1], 'k-', label='random')
plt.plot([0, 0, 1, 1], [0, 1, 1, 1], 'g-', label='perfect')
plt.legend()
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.show()

from sklearn.metrics import roc_auc_score
import matplotlib.pyplot as plt

auc_RF = roc_auc_score(df.actual_label.values, df.model_RF.values)
auc_LR = roc_auc_score(df.actual_label.values, df.model_LR.values)

print('AUC RF: %.3f' % auc_RF)
print('AUC LR: %.3f' % auc_LR)

plt.plot(fpr_RF, tpr_RF, 'r-', label='RF AUC: %.3f' % auc_RF)
plt.plot(fpr_LR, tpr_LR, 'b-', label='LR AUC: %.3f' % auc_LR)
plt.plot([0, 1], [0, 1], 'k-', label='random')
plt.plot([0, 0, 1, 1], [0, 1, 1, 1], 'g-', label='perfect')
plt.legend()
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.show()